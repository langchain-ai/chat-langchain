{"inputs": {"question": "What are the benefits of using LangChain for building chatbots?"}, "outputs": {"answer": "LangChain offers several benefits for building chatbots, including modular components for handling various tasks, easy integration with multiple language models, built-in support for memory and state management, and a flexible architecture that allows for customization and scalability."}, "metadata": {"id": "qa_001", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "How do I use LangChain with multiple LLMs?"}, "outputs": {"answer": "You can use multiple LLMs in LangChain by defining a chain that includes different LLMs as components. You can route inputs to specific LLMs based on conditions or combine their outputs using techniques like ensemble methods or voting."}, "metadata": {"id": "qa_002", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "Can I use custom embeddings with LangChain?"}, "outputs": {"answer": "Yes, you can use custom embeddings with LangChain by creating an instance of your embedding model and passing it to the vector store. Ensure that your embeddings are compatible with the expected input format of the vector store."}, "metadata": {"id": "qa_003", "category": "retrieval", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "What is the best way to handle API response errors in LangChain?"}, "outputs": {"answer": "In LangChain, you can handle API response errors by implementing error handling logic within your agent or chain. Use try-except blocks to catch exceptions and define fallback strategies, such as switching to a backup model or retrying the request after a delay."}, "metadata": {"id": "qa_004", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api", "best-practices", "error-handling"]}}
{"inputs": {"question": "How can I improve the performance of my LangChain model?"}, "outputs": {"answer": "To improve the performance of your LangChain model, consider optimizing your prompt, refining your data preprocessing steps, and adjusting hyperparameters such as temperature and max tokens. Additionally, using a more powerful language model or fine-tuning your existing model on relevant data may yield better results."}, "metadata": {"id": "qa_005", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["performance"]}}
{"inputs": {"question": "How can I parallelize calls in LangChain?"}, "outputs": {"answer": "To make parallel calls from a LangChain object, use the 'batch()' (or asynchronous 'abatch()') method. You can also use a `RunnableParallel` object."}, "metadata": {"id": "qa_006", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api", "performance", "streaming"]}}
{"inputs": {"question": "in code, how can i add a system message at the end of the conversation history to influence the output of the llm"}, "outputs": {"answer": "```python from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder prompt_template = ChatPromptTemplate.from_messages( [ (\"human\", \"{user_input}\"), MessagesPlaceholder(variable_name=\"chat_history\") (\"system\", \"New System Instruction here\"), ] ) ``` When formatted, it will generate 1 human/user message and 1 system message, as well as messages for the whole conversation history. ```python prompt_template.format_messages(user_input=\"Hello\", chat_history=[]) ``` [HumanMessage(content='Hello', additional_kwargs={}, example=False), SystemMessage(content='New System Instruction here', additional_kwargs={})]"}, "metadata": {"id": "qa_007", "category": "memory", "difficulty": "intermediate", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example"]}}
{"inputs": {"question": "how to use enums in Tools that will be used by functions agent?"}, "outputs": {"answer": "If you're defining a dynamic tool (either using the @tool decorator or the `.from_function(...) classmethod), simply using the enum typehint is enough. If you are manually specifying an `args_schema` as a pydantic basemodel, you can specify the enum as the argument typehint there."}, "metadata": {"id": "qa_008", "category": "agents", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "How do I deal with openai rate limiting by having a backup model? Show me with code."}, "outputs": {"answer": "A straightforward way to do this is to specify fallback models using LangChain Expression Language. Example below: ```python from langchain.chat_models import ChatOpenAI bad_llm = ChatOpenAI(model_name=\"gpt-fake\") good_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\") llm = bad_llm.with_fallbacks([good_llm]) llm.invoke(\"Why did the the chicken cross the road?\") ```"}, "metadata": {"id": "qa_009", "category": "llms", "difficulty": "intermediate", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example", "error-handling"]}}
{"inputs": {"question": "I'm runing my own model using vllm. How do I connect it to LangChain?"}, "outputs": {"answer": "LangChain has a VLLM integration:\" ```python from langchain.llms import VLLM llm = VLLM(model=\"mosaicml/mpt-7b\", trust_remote_code=True, # mandatory for hf models max_new_tokens=128, top_k=10, top_p=0.95, temperature=0.8, ) print(llm(\"What is the capital of France ?\"))"}, "metadata": {"id": "qa_010", "category": "chains", "difficulty": "intermediate", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example", "api", "integration"]}}
{"inputs": {"question": "What's a runnable lambda?"}, "outputs": {"answer": "A runnable lambda is an object that implements LangChain's `Runnable` interface and runs a callable (i.e., a function). Note the function must accept a single argument."}, "metadata": {"id": "qa_011", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "What's an LLMChain"}, "outputs": {"answer": "An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output."}, "metadata": {"id": "qa_012", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "What's the difference between a prompt template and a chat prompt template?"}, "outputs": {"answer": "A prompt template wraps instructions, examples, and other formatting information for the llm's prompts. A chat prompt template provides similar functionality for chat models. The largest difference is that chat prompt templates expose a `format_messages` method to convert the prompt template into a list of structured chat messages."}, "metadata": {"id": "qa_013", "category": "prompts", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "What is ConversationSummaryBufferMemory?"}, "outputs": {"answer": "ConversationSummaryBufferMemory is a memory type that combines the ideas of keeping a buffer of recent interactions and compiling them into a summary. It uses token length to determine when to flush interactions."}, "metadata": {"id": "qa_014", "category": "memory", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "What does LangChain offer for token counting?"}, "outputs": {"answer": "LangChain offers a context manager that allows you to count tokens, as well as integrations with services like LangSmith that can help track tokens."}, "metadata": {"id": "qa_015", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "What is the purpose of caching embeddings?"}, "outputs": {"answer": "Caching embeddings allows for storing or temporarily caching embeddings to avoid the need for recomputing them."}, "metadata": {"id": "qa_016", "category": "retrieval", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "What is the purpose of the Retry parser?"}, "outputs": {"answer": "The Retry parser is used when the LLM output is not in the expected format. It passes the input and initial output back to the LLM so it can try again with the correct format."}, "metadata": {"id": "qa_017", "category": "general", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "What does ConversationBufferWindowMemory do?"}, "outputs": {"answer": "ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."}, "metadata": {"id": "qa_018", "category": "memory", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "What is the main purpose of the time-weighted vector store retriever?"}, "outputs": {"answer": "The main purpose of the time-weighted vector store retriever is to use a combination of semantic similarity and a time decay to retrieve relevant vectors."}, "metadata": {"id": "qa_019", "category": "retrieval", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "What serialization format is used to serialize chains to and from disk?"}, "outputs": {"answer": "The serialization format used is JSON or YAML."}, "metadata": {"id": "qa_020", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "whats an agent type that works well with Anthropic's models and supports structured output?"}, "outputs": {"answer": "The XML agent works well with Anthropic's Claude 2 model. Here's some code for doing so: ```python from langchain.agents import XMLAgent, tool, AgentExecutor from langchain.chat_models import ChatAnthropic from langchain.chains import LLMChain model = ChatAnthropic(model='claude-2') chain = LLMChain( llm=model, prompt=XMLAgent.get_default_prompt(), output_parser=XMLAgent.get_default_output_parser() ) agent = XMLAgent(tools=tool_list, llm_chain=chain) agent_executor = AgentExecutor(agent=agent, tools=tool_list, verbose=True) ```"}, "metadata": {"id": "qa_021", "category": "agents", "difficulty": "intermediate", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example"]}}
{"inputs": {"question": "I want to save the configuration for a given LLM. Show me how to do that."}, "outputs": {"answer": "To save the configuration for a given LLM, you can follow these steps: First, let's go over loading an LLM configuration from disk. LLM configurations can be saved in two formats: JSON or YAML, but they are loaded in the same way. For a JSON-formatted LLM configuration (llm.json): ```json { \"model_name\": \"text-davinci-003\", \"temperature\": 0.7, \"max_tokens\": 256, \"top_p\": 1.0, \"frequency_penalty\": 0.0, \"presence_penalty\": 0.0, \"n\": 1, \"best_of\": 1, \"request_timeout\": null, \"_type\": \"openai\" } ``` Load the configuration like this: ```python from langchain.llms.loading import load_llm llm = load_llm(\"llm.json\") ``` For a YAML-formatted LLM configuration (llm.yaml): ```yaml _type: openai best_of: 1 frequency_penalty: 0.0 max_tokens: 256 model_name: text-davinci-003 n: 1 presence_penalty: 0.0 request_timeout: null temperature: 0.7 top_p: 1.0 ``` Load the configuration like this: ```python from langchain.llms.loading import load_llm llm = load_llm(\"llm.yaml\") ``` To save an LLM configuration from memory to a serialized version, you can use the `.save` method. This method supports both JSON and YAML formats: ```python llm.save(\"llm.json\") llm.save(\"llm.yaml\") ``` This way, you can easily save and load LLM configurations to and from disk."}, "metadata": {"id": "qa_022", "category": "llms", "difficulty": "advanced", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example", "api", "configuration"]}}
{"inputs": {"question": "Show me an example using Weaviate, but customizing the VectorStoreRetriever to return the top 10 k nearest neighbors."}, "outputs": {"answer": "To customize the Weaviate client and return the top 10 k nearest neighbors, you can utilize the `as_retriever` method with the appropriate parameters. Here's how you can achieve this: ```python # Assuming you have imported the necessary modules and classes # Create the Weaviate client client = weaviate.Client(url=os.environ[\"WEAVIATE_URL\"], ...) # Initialize the Weaviate wrapper weaviate = Weaviate(client, index_name, text_key) # Customize the client to return top 10 k nearest neighbors using as_retriever custom_retriever = weaviate.as_retriever( search_type=\"similarity\", search_kwargs={ 'k': 10 # Customize the value of k as needed } ) # Now you can use the custom_retriever to perform searches results = custom_retriever.search(query, ...) ```"}, "metadata": {"id": "qa_023", "category": "retrieval", "difficulty": "intermediate", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example", "api"]}}
{"inputs": {"question": "What is BabyAGI"}, "outputs": {"answer": "BabyAGI is an example of an \"autonomous AI agent\" that can generate and simulate the execution of tasks based on a given objective."}, "metadata": {"id": "qa_024", "category": "agents", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "What is the difference between ChatPromptTemplate and PromptTemplate?"}, "outputs": {"answer": "ChatPromptTemplate and PromptTemplate are similar in their functions. PromptTemplate takes a prompt as a string that can contain variables. ChatPromptTemplate separates the prompt into structured messages of different kinds. Each chat message is associated with content, and an additional parameter called role. Some example roles include system, human (or user), and AI (or assistant)."}, "metadata": {"id": "qa_025", "category": "prompts", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "Show me how to use RecursiveURLLoader"}, "outputs": {"answer": "The RecursiveURLLoader comes from the langchain.document_loaders.recursive_url_loader module. Here's an example of how to use it: ```python from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader # Create an instance of RecursiveUrlLoader with the URL you want to load loader = RecursiveUrlLoader(url=\"https://example.com\") # Load all child links from the URL page documents = loader.load() # Fetch the documents for doc in documents: print(doc) ``` Make sure to replace \"https://example.com\" with the actual URL you want to load."}, "metadata": {"id": "qa_026", "category": "document_loaders", "difficulty": "intermediate", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example"]}}
{"inputs": {"question": "what are the main methods supported by Runnables"}, "outputs": {"answer": "The `runnable` interface in LangChain supports several main methods for interacting with components: 1. `stream`: This method streams back response chunks in real-time, useful for processing large computations or text generation as they become available. 2. `invoke`: Calls the runnable chain on a single input, providing a single response output. 3. `batch`: Calls the runnable chain on a list of inputs, returning a list of corresponding responses. 4. `astream`: Asynchronously streams response chunks back, suitable for handling concurrent tasks. 5. `ainvoke`: Asynchronously calls the runnable chain on a single input and returns the response. 6. `abatch`: Asynchronously calls the runnable chain on a list of inputs, returning a list of corresponding responses."}, "metadata": {"id": "qa_027", "category": "chains", "difficulty": "intermediate", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api", "streaming"]}}
{"inputs": {"question": "What is html2texttransformer? Does it omit urls?"}, "outputs": {"answer": "The `Html2TextTransformer` is a class provided by LangChain's `document_transformers` module. It is used to transform HTML content into clean, easy-to-read plain ASCII text. This transformer is designed to convert HTML pages into human-readable text, making it suitable for scenarios where the goal is to extract content without the need to manipulate specific HTML elements. If urls are present in the source Document's metadata, then they will be retained. If they are stored in the page_content itself, the class makes no guarantees that they will be retained."}, "metadata": {"id": "qa_028", "category": "document_loaders", "difficulty": "intermediate", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "I want to return the source documents of my Weaviate retriever. Show me how"}, "outputs": {"answer": "To retrieve the source documents from your Weaviate vectorstore, you need to configure your client to include the 'sources' attribute. Here's an example of how you can achieve this: ```python weaviate_client = Weaviate( client=client, index_name='index_name', text_key='text', embedding=OpenAIEmbeddings(), by_text=False, attributes=['source'], ) ``` By including the 'source' attribute in the attributes list, you ensure that the source documents are returned when you interact with the Weaviate client."}, "metadata": {"id": "qa_029", "category": "retrieval", "difficulty": "intermediate", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example", "configuration"]}}
{"inputs": {"question": "what is RAG"}, "outputs": {"answer": "RAG stands for Retrieval Augmented Generation, which refers to the application design of using an LLM to generate responses to user inputs grounded on retrieved information. The retrieved information can come from anywhere, though it typically is fetched from 1 or more databases, search engines, knowledge graphs, or other sources."}, "metadata": {"id": "qa_030", "category": "retrieval", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "whats the code to load text file into a vector store"}, "outputs": {"answer": "To load a text file into a vector store using LangChain's document loaders, you can use the following code snippet: ```python from langchain.document_loaders import TextLoader loader = TextLoader(\"./index.txt\") docs = loader.load() print(docs) ``` This code imports the `TextLoader` class from the document_loaders module, creates an instance of the loader with the path to the text file as an argument, and then uses the `load()` method to read the file's content and create a Document. The resulting Document will contain the text content of the file."}, "metadata": {"id": "qa_031", "category": "retrieval", "difficulty": "intermediate", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example", "api"]}}
{"inputs": {"question": "Help me debug: TypeError: Pinecone.similarity_search_with_score() got an unexpected keyword argument 'min_score'"}, "outputs": {"answer": "Your error is likely due to an incorrect parameter being passed to the 'similarity_search_with_score' function in the Pinecone vector store. The correct function signature does not include a 'min_score' argument. This function retrieves documents most similar to a given query text while providing both the documents and their corresponding similarity scores. To use 'similarity_search_with_score', ensure you provide the right arguments: 'query' for the text you want to find similar documents for, 'k' for the number of documents to retrieve (default: 4), 'filter' as an optional metadata-based filtering dictionary, and 'namespace' to specify the search namespace (default is the empty namespace ''). The function returns a list of tuples, each containing a Document and a similarity score (float). To resolve the 'TypeError' issue, ensure proper argument usage as per the documentation."}, "metadata": {"id": "qa_032", "category": "retrieval", "difficulty": "intermediate", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api", "error-handling"]}}
{"inputs": {"question": "How can I create a simple chat model using my locally saved huggingface model"}, "outputs": {"answer": "Certainly! To create a simple chat model using your locally saved Hugging Face model, follow these steps: 1. Install the necessary Python libraries: - Install 'transformers' library for working with models and tokenizers using 'pip install transformers'. 2. Import the `HuggingFacePipeline` class to work with your local pipeline ``` from langchain.llms import HuggingFacePipeline # Create an instance of the wrapper llm = HuggingFacePipeline.from_model_id(\"path/to/model_directory\", task=\"text-generation\") ``` Alternatively, load your pipeline directly: ``` from langchain.llms.huggingface_pipeline import HuggingFacePipeline from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline model_path = \"path/to/model_dir\" tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path) pipe = pipeline( \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10 ) llm = HuggingFacePipeline(pipeline=pipe) ``` Finally, interact with the chat model ``` response = llm.invoke('User: Hello, how can I help?') ```"}, "metadata": {"id": "qa_033", "category": "llms", "difficulty": "advanced", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example", "configuration"]}}
{"inputs": {"question": "How do I use a React Agent with an Anthropic model?"}, "outputs": {"answer": "In order to use a ReAct Agent with an Anthropic model, you should first install the required packages: pip install anthropic Next, import the necessary classes and functions: from langchain.agents import load_tools from langchain.agents import initialize_agent from langchain.agents import AgentType from langchain.chat_models import ChatAnthropic Next, create an instance of ChatAnthropic. Make sure you have set your API key in an environment variable called ANTHROPIC_API_KEY or pass anthropic_api_key as a parameter. chat = ChatAnthropic() Next, load some tools for your agent to use: # Specify tools (these can be any custom tool, the following are examples) tools = load_tools([\"serpapi\", \"llm-math\"], llm=chat) Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use. agent = initialize_agent(tools, chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True) Now, enter a prompt to test it out: agent.invoke(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"}, "metadata": {"id": "qa_034", "category": "agents", "difficulty": "advanced", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api", "configuration", "best-practices"]}}
{"inputs": {"question": "How do I use Qdrant as a vector store in the conversational retrieval chain?"}, "outputs": {"answer": "In order to use Qdrant as a vector store in the conversational retrieval chain, you should start by installing Qdrant: pip install qdrant-client Next, import the necessary classes and functions: from langchain.vectorstores import Qdrant from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.llms import OpenAI from langchain.chains import ConversationalRetrievalChain from langchain.document_loaders import TextLoader from langchain.memory import ConversationBufferMemory Load, split, and embed your documents: loader = TextLoader(\"state_of_the_union.txt\") documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\" \") documents = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() Next, connect to Qdrant. This is for storing locally on-disk, but you can also use an on-premise server deployment or store on the Qdrant cloud: qdrant = Qdrant.from_documents( documents, embeddings, path=\"/tmp/local_qdrant\", collection_name=\"my_documents\", ) Next, create a memory object, which is necessary to track the inputs/outputs and hold a conversation: memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) Now initialize the ConversationalRetrievalChain: qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), qdrant.as_retriever(), memory=memory) Now, you can create queries to test it out: query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"question\": query}) Output: The president said that he had nominated Ketanji Brown Jackson to serve on the United States Supreme Court four days ago. query = \"Who are they succeeding?\" result = qa({\"question\": query}) Output: Justice Breyer."}, "metadata": {"id": "qa_035", "category": "memory", "difficulty": "advanced", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api", "configuration", "best-practices"]}}
{"inputs": {"question": "How do I use Summary Memory in the conversational retrieval chain?"}, "outputs": {"answer": "In order to use Summary Memory in the conversational retrieval chain, you should first import the necessary classes and functions: from langchain.memory import ConversationSummaryMemory from langchain.embeddings.openai import OpenAIEmbeddings from langchain.vectorstores import Chroma # You can choose any LangChain vectorstore from langchain.text_splitter import CharacterTextSplitter from langchain.chat_models import ChatOpenAI # You can choose any LangChain vectorstore from langchain.chains import ConversationalRetrievalChain from langchain.document_loaders import TextLoader Split and embed your documents, and add them to your selected vectorstore: loader = TextLoader(\"state_of_the_union.txt\") documents = loader.load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\" \") documents = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() vectorstore = Chroma.from_documents(documents, embeddings) Next, create a ConversationSummaryMemory object: memory = ConversationSummaryMemory(llm=ChatOpenAI(model=\"gpt-3.5-turbo\"), memory_key=\"chat_history\", return_messages=True) Now initialize the ConversationalRetrievalChain: qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0), vectorstore.as_retriever(), memory=memory) Now, you can create queries to test it out: query = \"What did the president say about Ketanji Brown Jackson\" result = qa({\"question\": query}) Output: The president said that he had nominated Ketanji Brown Jackson to serve on the United States Supreme Court four days ago. query = \"Who are they succeeding?\" result = qa.invoke({\"question\": query}) Output: Justice Breyer."}, "metadata": {"id": "qa_036", "category": "memory", "difficulty": "advanced", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api", "best-practices"]}}
{"inputs": {"question": "How do I load Youtube transcripts and CSV documents?"}, "outputs": {"answer": "To load YouTube transcripts in LangChain, you can use the YoutubeLoader document loader. Here's an example of how to do it: Install the required packages: ``` pip install langchain youtube_transcript_api pytube ``` Import the necessary classes: from langchain.document_loaders import YoutubeLoader Create an instance of the YoutubeLoader class and provide the YouTube video URL: loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=QsYGlZkevEg\", add_video_info=True) Load the transcript using the load method: video = loader.load() To load CSV documents you can use CSVLoader. Follow the steps below: Import the necessary classes: from langchain.document_loaders import CSVLoader Create an instance of the CSVLoader class and provide the path to the CSV file: loader = CSVLoader(file_path=\"./example_data/mlb_teams_2012.csv\") Load the CSV document using the load method: loader.load()"}, "metadata": {"id": "qa_037", "category": "document_loaders", "difficulty": "intermediate", "has_code": true, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["code-example", "api", "configuration"]}}
{"inputs": {"question": "my agent keeps getting an OutputParserException is something i can set to make it take care of these?"}, "outputs": {"answer": "You can control the agent's behavior for output parsing errors by initializing the agent with `handle_parsing_errors`=True. Then the agent will see 'Handle errors with Invalid or incomplete response'. You can also set a string to provide a custom error message to the agent, or provide a custom error handling function."}, "metadata": {"id": "qa_038", "category": "agents", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api", "error-handling"]}}
{"inputs": {"question": "What does ReAct mean?"}, "outputs": {"answer": "ReAct in LangChain refers to a type of agent that 'Reasons' and 'Act's in a loop."}, "metadata": {"id": "qa_039", "category": "agents", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "What are intermediate steps in langchain?"}, "outputs": {"answer": "Intermediate steps represent previous agent actions and corresponding outputs. They are used to help the agent know what work has already been done in pursuit of a goal."}, "metadata": {"id": "qa_040", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "im getting a rate limit error for my llm - how do I have it automatically go to a different model?"}, "outputs": {"answer": "To switch to a different model when an LLM is getting rate limited, you can use 'fallbacks'. For instance, if you have two llms: openai_llm and anthropic_llm, you can have the program fall back to the anthropic llm by calling `llm = openai_llm.with_fallbacks([anthropic_llm])`"}, "metadata": {"id": "qa_041", "category": "llms", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["error-handling"]}}
{"inputs": {"question": "What's function calling"}, "outputs": {"answer": "Function calling refers to the ability to define functions for LLMs to let them choose to call with structured arguments."}, "metadata": {"id": "qa_042", "category": "general", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "what is the langserve"}, "outputs": {"answer": "LangServe is an easy way to deploy any LangChain runnable, chain, or agent. It uses FastAPI and pydantic to automatically create an API server from any compatible LangChain component."}, "metadata": {"id": "qa_043", "category": "deployment", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "What's a string evaluator"}, "outputs": {"answer": "A string evaluator in LangChain is an object that assesses the performance of a language model's predicted response by comparing it to an input string and/or reference string."}, "metadata": {"id": "qa_044", "category": "evaluation", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["performance"]}}
{"inputs": {"question": "whats a toolkit"}, "outputs": {"answer": "Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods."}, "metadata": {"id": "qa_045", "category": "agents", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "is langchain compatible with pydantic v2?"}, "outputs": {"answer": "As of langchain>=0.0.267, LangChain will allow users to install either Pydantic V1 or V2, though it internally will use pydantic V1."}, "metadata": {"id": "qa_046", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["configuration"]}}
{"inputs": {"question": "how many llm api calls are made in OpenAIFunctionsAgent"}, "outputs": {"answer": "The OpenAIFunctionsAgent is an agent driven by OpenAI's function-powered API. It uses the function_call parameter from OpenAI to determine which tools to use for each API call. In general, the number of API calls made by the OpenAIFunctionsAgent depends on the question and the number of tools used in the agent's pipeline. At minimum, one call will be made to the LLM to (optionally) select a tool or directly respond. For more complex questions or intents, the agent may make many calls."}, "metadata": {"id": "qa_047", "category": "agents", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "What are some ways of doing retrieval augmented generation?"}, "outputs": {"answer": "Retrieval Augmented Generation (RAG) is a process that involves retrieving external data and using it in the generation step of language models. LangChain provides various building blocks for RAG applications. Here are some ways of doing retrieval augmented generation: - Document loaders: LangChain offers over 100 document loaders to load documents from different sources, including private S3 buckets and public websites. - Document transformers: These help in fetching relevant parts of documents by splitting or chunking large documents into smaller chunks. - Text embedding models: LangChain integrates with over 25 different embedding providers and methods to create embeddings for documents, allowing efficient similarity-based retrieval. - Vector stores: With the rise of embeddings, LangChain supports over 50 vector stores for efficient storage and searching of embeddings. - Retrievers: LangChain supports various retrieval algorithms, including semantic search, parent document retriever, self-query retriever, ensemble retriever, and more, to retrieve the data from the database. These components work together to enable retrieval augmented generation in LangChain. [0] [1]"}, "metadata": {"id": "qa_048", "category": "retrieval", "difficulty": "advanced", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api", "integration"]}}
{"inputs": {"question": "whats the difference between a handler and an inheritable_handler?"}, "outputs": {"answer": "In Langchain, both `handler` and `inheritable_handler` are types of callback handlers. Handlers can refer to inheritable handlers or local handlers. Inheritable handlers are propagated to other objects that are called within a Runnable, chain, tool, or other object. Local handlers are only called by the object they are assigned to and not called by any other objects."}, "metadata": {"id": "qa_049", "category": "general", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": []}}
{"inputs": {"question": "What is a chain?"}, "outputs": {"answer": "A 'Chain' is a class in LangChain that composes a number of components together into a single object. An example would be the LLMChain, which formats an input as a prompt, calls an LLM, and then parses the output. \"chaining\" also can generically refer to composing llms, functions, and other operations together into a larger program."}, "metadata": {"id": "qa_050", "category": "chains", "difficulty": "beginner", "has_code": false, "created_at": "2023-11-08T03:20:22Z", "source": "langchain_docs", "tags": ["api"]}}
{"inputs": {"question": "What is context engineering in LangChain agents?"}, "outputs": {"answer": "Context engineering is the practice of providing the right information and tools in the right format so the LLM can accomplish a task. It involves controlling what goes into model calls, what tools can access, and what happens between steps in the agent lifecycle. This is considered the number one job of AI Engineers for building reliable agents."}, "metadata": {"id": "qa_006", "category": "agents", "difficulty": "intermediate", "has_code": false, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents"]}}
{"inputs": {"question": "Why do agents fail in production?"}, "outputs": {"answer": "Agents typically fail for one of two reasons: either the underlying LLM is not capable enough, or the right context was not passed to the LLM. More often than not, it's the second reason - lack of proper context - that causes agents to not be reliable. This is why context engineering is crucial for building production-ready agents."}, "metadata": {"id": "qa_007", "category": "agents", "difficulty": "intermediate", "has_code": false, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents", "best-practices"]}}
{"inputs": {"question": "What are the three types of context in LangChain agents?"}, "outputs": {"answer": "The three types of context in LangChain agents are: 1) Model Context - what goes into model calls including instructions, message history, tools, and response format (transient), 2) Tool Context - what tools can access and produce, including reads and writes to state, store, and runtime context (persistent), and 3) Life-cycle Context - what happens between model and tool calls such as summarization, guardrails, and logging (persistent)."}, "metadata": {"id": "qa_008", "category": "agents", "difficulty": "intermediate", "has_code": false, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents", "architecture"]}}
{"inputs": {"question": "What is the difference between transient and persistent context?"}, "outputs": {"answer": "Transient context is what the LLM sees for a single call - you can modify messages, tools, or prompts without changing what's saved in state. Persistent context is what gets saved in state across turns - life-cycle hooks and tool writes modify this permanently and it persists across the conversation."}, "metadata": {"id": "qa_009", "category": "agents", "difficulty": "intermediate", "has_code": false, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents", "state-management"]}}
{"inputs": {"question": "What are the three data sources agents access in LangChain?"}, "outputs": {"answer": "Agents access three data sources: 1) Runtime Context (static configuration) - conversation-scoped data like user ID, API keys, database connections, permissions, and environment settings, 2) State (short-term memory) - conversation-scoped data like current messages, uploaded files, authentication status, and tool results, and 3) Store (long-term memory) - cross-conversation data like user preferences, extracted insights, memories, and historical data."}, "metadata": {"id": "qa_010", "category": "agents", "difficulty": "intermediate", "has_code": false, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents", "memory"]}}
{"inputs": {"question": "How does middleware enable context engineering in LangChain?"}, "outputs": {"answer": "Middleware in LangChain allows you to hook into any step in the agent lifecycle to either update context (modify state and store to persist changes, update conversation history, or save insights) or jump in the lifecycle (move to different steps in the agent cycle based on context, such as skipping tool execution if a condition is met or repeating model calls with modified context)."}, "metadata": {"id": "qa_011", "category": "agents", "difficulty": "advanced", "has_code": false, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents", "middleware"]}}
{"inputs": {"question": "What components make up model context in LangChain agents?"}, "outputs": {"answer": "Model context consists of five components: 1) System Prompt - base instructions from developer to LLM, 2) Messages - full conversation history sent to LLM, 3) Tools - utilities the agent has access to for taking actions, 4) Model - the actual model and its configuration to be called, and 5) Response Format - schema specification for the model's final response. All of these can draw from state, store, or runtime context."}, "metadata": {"id": "qa_012", "category": "agents", "difficulty": "intermediate", "has_code": false, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents", "model-context"]}}
{"inputs": {"question": "How can I implement automatic conversation summarization in LangChain?"}, "outputs": {"answer": "Use the built-in summarizationMiddleware by adding it to your agent configuration. You can specify parameters like maxTokensBeforeSummary to trigger summarization when conversation exceeds a token limit, and messagesToKeep to retain recent messages. When triggered, it automatically summarizes older messages using a separate LLM call, replaces them with a summary message in State permanently, and keeps recent messages intact for context."}, "metadata": {"id": "qa_013", "category": "agents", "difficulty": "advanced", "has_code": true, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents", "middleware", "summarization"]}}
{"inputs": {"question": "What is the agent loop in LangChain?"}, "outputs": {"answer": "The agent loop consists of two main steps that repeat until the LLM decides to finish: 1) Model call - calls the LLM with a prompt and available tools, returns either a response or a request to execute tools, and 2) Tool execution - executes the tools that the LLM requested and returns tool results. This cycle continues iteratively to accomplish the given task."}, "metadata": {"id": "qa_014", "category": "agents", "difficulty": "beginner", "has_code": false, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents", "architecture"]}}
{"inputs": {"question": "How can tools write to State in LangChain?"}, "outputs": {"answer": "Tools can write to State using the Command object. When a tool needs to update state, it returns a Command with an update parameter containing the state changes. For example, after authenticating a user, a tool can return Command with update set to authenticated status, which permanently modifies the conversation state."}, "metadata": {"id": "qa_015", "category": "agents", "difficulty": "advanced", "has_code": true, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents", "tools", "state-management"]}}
{"inputs": {"question": "What are the best practices for context engineering in LangChain?"}, "outputs": {"answer": "Key best practices include: 1) Start simple with static prompts and tools, adding dynamics only when needed, 2) Test incrementally by adding one context engineering feature at a time, 3) Monitor performance tracking model calls, token usage, and latency, 4) Use built-in middleware like SummarizationMiddleware and LLMToolSelectorMiddleware, 5) Document your context strategy clearly, and 6) Understand the difference between transient (per-call) and persistent (saved to state) context changes."}, "metadata": {"id": "qa_016", "category": "agents", "difficulty": "intermediate", "has_code": false, "created_at": "2024-11-18T10:00:00Z", "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering", "tags": ["context-engineering", "agents", "best-practices"]}}