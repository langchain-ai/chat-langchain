[
  {
    "page_content": "Agents - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationCore componentsAgentsLangChainLangGraphIntegrationsLearnReferenceContributeTypeScriptOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityEnglishcloseOn this pageCore componentsModelStatic modelDynamic modelToolsDefining toolsTool error handlingTool use in the ReAct loopSystem promptDynamic system promptInvocationAdvanced conceptsStructured outputMemoryStreamingMiddlewareCore componentsAgentsCopy pageCopy pageAgents combine language models with tools to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions.\ncreateAgent() provides a production-ready agent implementation.\nAn LLM Agent runs tools in a loop to achieve a goal.\nAn agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached.\n\ncreateAgent() builds a graph-based agent runtime using LangGraph. A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware.Learn more about the Graph API.\n​Core components\n​Model\nThe model is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection.\n​Static model\nStatic models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach.\nTo initialize a static model from a model identifier string:\nCopyAsk AIimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-5\",\n  tools: []\n});\n\nModel identifier strings use the format provider:model (e.g. \"openai:gpt-5\"). You may want more control over the model configuration, in which case you can initialize a model instance directly using the provider package:\nCopyAsk AIimport { createAgent } from \"langchain\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0.1,\n  maxTokens: 1000,\n  timeout: 30\n});\n\nconst agent = createAgent({\n  model,\n  tools: []\n});\n\nModel instances give you complete control over configuration. Use them when you need to set specific parameters like temperature, max_tokens, timeouts, or configure API keys, base_url, and other provider-specific settings. Refer to the API reference to see available params and methods on your model.\n​Dynamic model\nDynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization.\nTo use a dynamic model, create middleware with wrapModelCall that modifies the model in the request:\nCopyAsk AIimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createAgent, createMiddleware } from \"langchain\";\n\nconst basicModel = new ChatOpenAI({ model: \"gpt-4o-mini\" });\nconst advancedModel = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst dynamicModelSelection = createMiddleware({\n  name: \"DynamicModelSelection\",\n  wrapModelCall: (request, handler) => {\n    // Choose model based on conversation complexity\n    const messageCount = request.messages.length;\n\n    return handler({\n        ...request,\n        model: messageCount > 10 ? advancedModel : basicModel,\n    });\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o-mini\", // Base model (used when messageCount ≤ 10)\n  tools,\n  middleware: [dynamicModelSelection] as const,\n});\n\nFor more details on middleware and advanced patterns, see the middleware documentation.\nFor model configuration details, see Models. For dynamic model selection patterns, see Dynamic model in middleware.\n​Tools\nTools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating:\n\nMultiple tool calls in sequence (triggered by a single prompt)\nParallel tool calls when appropriate\nDynamic tool selection based on previous results\nTool retry logic and error handling\nState persistence across tool calls\n\nFor more information, see Tools.\n​Defining tools\nPass a list of tools to the agent.\nCopyAsk AIimport * as z from \"zod\";\nimport { createAgent, tool } from \"langchain\";\n\nconst search = tool(\n  ({ query }) => `Results for: ${query}`,\n  {\n    name: \"search\",\n    description: \"Search for information\",\n    schema: z.object({\n      query: z.string().describe(\"The query to search for\"),\n    }),\n  }\n);\n\nconst getWeather = tool(\n  ({ location }) => `Weather in ${location}: Sunny, 72°F`,\n  {\n    name: \"get_weather\",\n    description: \"Get weather information for a location\",\n    schema: z.object({\n      location: z.string().describe(\"The location to get weather for\"),\n    }),\n  }\n);\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [search, getWeather],\n});\n\nIf an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities.\n​Tool error handling\nTo customize how tool errors are handled, use the wrapToolCall hook in a custom middleware:\nCopyAsk AIimport { createAgent, createMiddleware, ToolMessage } from \"langchain\";\n\nconst handleToolErrors = createMiddleware({\n  name: \"HandleToolErrors\",\n  wrapToolCall: (request, handler) => {\n    try {\n      return handler(request);\n    } catch (error) {\n      // Return a custom error message to the model\n      return new ToolMessage({\n        content: `Tool error: Please check your input and try again. (${error})`,\n        tool_call_id: request.toolCall.id!,\n      });\n    }\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [\n    /* ... */\n  ],\n  middleware: [handleToolErrors] as const,\n});\n\nThe agent will return a @[ToolMessage] with the custom error message when a tool fails.\n​Tool use in the ReAct loop\nAgents follow the ReAct (“Reasoning + Acting”) pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.\nExample of ReAct loopPrompt: Identify the current most popular wireless headphones and verify availability.CopyAsk AI================================ Human Message =================================\n\nFind the most popular wireless headphones right now and check if they're in stock\n\nReasoning: “Popularity is time-sensitive, I need to use the provided search tool.”\nActing: Call search_products(\"wireless headphones\")\nCopyAsk AI================================== Ai Message ==================================\nTool Calls:\n  search_products (call_abc123)\n Call ID: call_abc123\n  Args:\n    query: wireless headphones\nCopyAsk AI================================= Tool Message =================================\n\nFound 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ...\n\nReasoning: “I need to confirm availability for the top-ranked item before answering.”\nActing: Call check_inventory(\"WH-1000XM5\")\nCopyAsk AI================================== Ai Message ==================================\nTool Calls:\n  check_inventory (call_def456)\n Call ID: call_def456\n  Args:\n    product_id: WH-1000XM5\nCopyAsk AI================================= Tool Message =================================\n\nProduct WH-1000XM5: 10 units in stock\n\nReasoning: “I have the most popular model and its stock status. I can now answer the user’s question.”\nActing: Produce final answer\nCopyAsk AI================================== Ai Message ==================================\n\nI found wireless headphones (model WH-1000XM5) with 10 units in stock...\n\nTo learn more about tools, see Tools.\n​System prompt\nYou can shape how your agent approaches tasks by providing a prompt. The systemPrompt parameter can be provided as a string:\nCopyAsk AIconst agent = createAgent({\n  model,\n  tools,\n  systemPrompt: \"You are a helpful assistant. Be concise and accurate.\",\n});\n\nWhen no @[system_prompt] is provided, the agent will infer its task from the messages directly.\n​Dynamic system prompt\nFor more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use middleware.\nCopyAsk AIimport * as z from \"zod\";\nimport { createAgent, dynamicSystemPromptMiddleware } from \"langchain\";\n\nconst contextSchema = z.object({\n  userRole: z.enum([\"expert\", \"beginner\"]),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [/* ... */],\n  contextSchema,\n  middleware: [\n    dynamicSystemPromptMiddleware<z.infer<typeof contextSchema>>((state, runtime) => {\n      const userRole = runtime.context.userRole || \"user\";\n      const basePrompt = \"You are a helpful assistant.\";\n\n      if (userRole === \"expert\") {\n        return `${basePrompt} Provide detailed technical responses.`;\n      } else if (userRole === \"beginner\") {\n        return `${basePrompt} Explain concepts simply and avoid jargon.`;\n      }\n      return basePrompt;\n    }),\n  ],\n});\n\n// The system prompt will be set dynamically based on context\nconst result = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"Explain machine learning\" }] },\n  { context: { userRole: \"expert\" } }\n);\n\nFor more details on message types and formatting, see Messages. For comprehensive middleware documentation, see Middleware.\n​Invocation\nYou can invoke an agent by passing an update to its State. All agents include a sequence of messages in their state; to invoke the agent, pass a new message:\nCopyAsk AIawait agent.invoke({\n  messages: [{ role: \"user\", content: \"What's the weather in San Francisco?\" }],\n})\n\nFor streaming steps and / or tokens from the agent, refer to the streaming guide.\nOtherwise, the agent follows the LangGraph Graph API and supports all associated methods.\n​Advanced concepts\n​Structured output\nIn some situations, you may want the agent to return an output in a specific format. LangChain provides a simple, universal way to do this with the responseFormat parameter.\nCopyAsk AIimport * as z from \"zod\";\nimport { createAgent } from \"langchain\";\n\nconst ContactInfo = z.object({\n  name: z.string(),\n  email: z.string(),\n  phone: z.string(),\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  responseFormat: ContactInfo,\n});\n\nconst result = await agent.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n    },\n  ],\n});\n\nconsole.log(result.structuredResponse);\n// {\n//   name: 'John Doe',\n//   email: 'john@example.com',\n//   phone: '(555) 123-4567'\n// }\n\nTo learn about structured output, see Structured output.\n​Memory\nAgents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation.\nInformation stored in the state can be thought of as the short-term memory of the agent:\nCopyAsk AIimport * as z from \"zod\";\nimport { MessagesZodState } from \"@langchain/langgraph\";\nimport { createAgent, type BaseMessage } from \"langchain\";\n\nconst customAgentState = z.object({\n  messages: MessagesZodState.shape.messages,\n  userPreferences: z.record(z.string(), z.string()),\n});\n\nconst CustomAgentState = createAgent({\n  model: \"gpt-4o\",\n  tools: [],\n  stateSchema: customAgentState,\n});\n\nTo learn more about memory, see Memory. For information on implementing long-term memory that persists across sessions, see Long-term memory.\n​Streaming\nWe’ve seen how the agent can be called with invoke to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur.\nCopyAsk AIconst stream = await agent.stream(\n  {\n    messages: [{\n      role: \"user\",\n      content: \"Search for AI news and summarize the findings\"\n    }],\n  },\n  { streamMode: \"values\" }\n);\n\nfor await (const chunk of stream) {\n  // Each chunk contains the full state at that point\n  const latestMessage = chunk.messages.at(-1);\n  if (latestMessage?.content) {\n    console.log(`Agent: ${latestMessage.content}`);\n  } else if (latestMessage?.tool_calls) {\n    const toolCallNames = latestMessage.tool_calls.map((tc) => tc.name);\n    console.log(`Calling tools: ${toolCallNames.join(\", \")}`);\n  }\n}\n\nFor more details on streaming, see Streaming.\n​Middleware\nMiddleware provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to:\n\nProcess state before the model is called (e.g., message trimming, context injection)\nModify or validate the model’s response (e.g., guardrails, content filtering)\nHandle tool execution errors with custom logic\nImplement dynamic model selection based on state or context\nAdd custom logging, monitoring, or analytics\n\nMiddleware integrates seamlessly into the agent’s execution graph, allowing you to intercept and modify data flow at key points without changing the core agent logic.\nFor comprehensive middleware documentation including hooks like beforeModel, afterModel, and wrapToolCall, see Middleware.\n\nEdit the source of this page on GitHub.\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoPhilosophyPreviousModelsNextDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify",
    "metadata": {
      "source": "https://docs.langchain.com/oss/javascript/langchain/agents",
      "title": "Agents - Docs by LangChain",
      "description": "",
      "language": "en",
      "loc": "https://docs.langchain.com/oss/javascript/langchain/agents",
      "lastmod": "2025-11-03T18:48:17.363Z"
    },
    "type": "Document"
  },
  {
    "page_content": "Context engineering in agents - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationAdvanced usageContext engineering in agentsLangChainLangGraphIntegrationsLearnReferenceContributeTypeScriptOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityEnglishcloseOn this pageOverviewWhy do agents fail?The agent loopWhat you can controlData sourcesHow it worksModel ContextSystem PromptMessagesToolsDefining toolsSelecting toolsModelResponse FormatDefining formatsSelecting formatsTool ContextReadsWritesLife-cycle ContextExample: SummarizationBest practicesRelated resourcesAdvanced usageContext engineering in agentsCopy pageCopy page​Overview\nThe hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.\n​Why do agents fail?\nWhen agents fail, it’s usually because the LLM call inside the agent took the wrong action / didn’t do what we expected. LLMs fail for one of two reasons:\n\nThe underlying LLM is not capable enough\nThe “right” context was not passed to the LLM\n\nMore often than not - it’s actually the second reason that causes agents to not be reliable.\nContext engineering is providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of “right” context is the number one blocker for more reliable agents, and LangChain’s agent abstractions are uniquely designed to facilitate context engineering.\nNew to context engineering? Start with the conceptual overview to understand the different types of context and when to use them.\n​The agent loop\nA typical agent loop consists of two main steps:\n\nModel call - calls the LLM with a prompt and available tools, returns either a response or a request to execute tools\nTool execution - executes the tools that the LLM requested, returns tool results\n\nThis loop continues until the LLM decides to finish.\n​What you can control\nTo build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps.\n\nContext TypeWhat You ControlTransient or Persistent\nModel ContextWhat goes into model calls (instructions, message history, tools, response format)TransientTool ContextWhat tools can access and produce (reads/writes to state, store, runtime context)PersistentLife-cycle ContextWhat happens between model and tool calls (summarization, guardrails, logging, etc.)Persistent\n\nTransient contextWhat the LLM sees for a single call. You can modify messages, tools, or prompts without changing what’s saved in state.Persistent contextWhat gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently.\n​Data sources\nThroughout this process, your agent accesses (reads / writes) different sources of data:\n\nData SourceAlso Known AsScopeExamples\nRuntime ContextStatic configurationConversation-scopedUser ID, API keys, database connections, permissions, environment settingsStateShort-term memoryConversation-scopedCurrent messages, uploaded files, authentication status, tool resultsStoreLong-term memoryCross-conversationUser preferences, extracted insights, memories, historical data\n\n​How it works\nLangChain middleware is the mechanism under the hood that makes context engineering practical for developers using LangChain.\nMiddleware allows you to hook into any step in the agent lifecycle and:\n\nUpdate context\nJump to a different step in the agent lifecycle\n\nThroughout this guide, you’ll see frequent use of the middleware API as a means to the context engineering end.\n​Model Context\nControl what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost.\nSystem PromptBase instructions from the developer to the LLM.MessagesThe full list of messages (conversation history) sent to the LLM.ToolsUtilities the agent has access to to take actions.ModelThe actual model (including configuration) to be called.Response FormatSchema specification for the model’s final response.\nAll of these types of model context can draw from state (short-term memory), store (long-term memory), or runtime context (static configuration).\n​System Prompt\nThe system prompt sets the LLM’s behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.\n State Store Runtime ContextAccess message count or conversation context from state:CopyAsk AIimport { createAgent } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [...],\n  middleware: [\n    dynamicSystemPromptMiddleware((state) => {\n      // Read from State: check conversation length\n      const messageCount = state.messages.length;\n\n      let base = \"You are a helpful assistant.\";\n\n      if (messageCount > 10) {\n        base += \"\\nThis is a long conversation - be extra concise.\";\n      }\n\n      return base;\n    }),\n  ],\n});\n\n​Messages\nMessages make up the prompt that is sent to the LLM.\nIt’s critical to manage the content of messages to ensure that the LLM has the right information to respond well.\n State Store Runtime ContextInject uploaded file context from State when relevant to current query:CopyAsk AIimport { createMiddleware } from \"langchain\";\n\nconst injectFileContext = createMiddleware({\n  name: \"InjectFileContext\",\n  wrapModelCall: (request, handler) => {\n    // request.state is a shortcut for request.state.messages\n    const uploadedFiles = request.state.uploadedFiles || [];  \n\n    if (uploadedFiles.length > 0) {\n      // Build context about available files\n      const fileDescriptions = uploadedFiles.map(file =>\n        `- ${file.name} (${file.type}): ${file.summary}`\n      );\n\n      const fileContext = `Files you have access to in this conversation:\n${fileDescriptions.join(\"\\n\")}\n\nReference these files when answering questions.`;\n\n      // Inject file context before recent messages\n      const messages = [  \n        ...request.messages  // Rest of conversation\n        { role: \"user\", content: fileContext }\n      ];\n      request = request.override({ messages });  \n    }\n\n    return handler(request);\n  },\n});\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [...],\n  middleware: [injectFileContext],\n});\n\nTransient vs Persistent Message Updates:The examples above use wrap_model_call to make transient updates - modifying what messages are sent to the model for a single call without changing what’s saved in state.For persistent updates that modify state (like the summarization example in Life-cycle Context), use life-cycle hooks like before_model or after_model to permanently update the conversation history. See the middleware documentation for more details.\n​Tools\nTools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.\n​Defining tools\nEach tool needs a clear name, description, argument names, and argument descriptions. These aren’t just metadata—they guide the model’s reasoning about when and how to use the tool.\nCopyAsk AIimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst searchOrders = tool(\n  async ({ userId, status, limit = 10 }) => {\n    // Implementation here\n  },\n  {\n    name: \"search_orders\",\n    description: `Search for user orders by status.\n\n    Use this when the user asks about order history or wants to check\n    order status. Always filter by the provided status.`,\n    schema: z.object({\n      userId: z.string().describe(\"Unique identifier for the user\"),\n      status: z.enum([\"pending\", \"shipped\", \"delivered\"]).describe(\"Order status to filter by\"),\n      limit: z.number().default(10).describe(\"Maximum number of results to return\"),\n    }),\n  }\n);\n\n​Selecting tools\nNot every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.\n State Store Runtime ContextEnable advanced tools only after certain conversation milestones:CopyAsk AIimport { createMiddleware } from \"langchain\";\n\nconst stateBasedTools = createMiddleware({\n  name: \"StateBasedTools\",\n  wrapModelCall: (request, handler) => {\n    // Read from State: check authentication and conversation length\n    const state = request.state;  \n    const isAuthenticated = state.authenticated || false;  \n    const messageCount = state.messages.length;\n\n    let filteredTools = request.tools;\n\n    // Only enable sensitive tools after authentication\n    if (!isAuthenticated) {\n      filteredTools = request.tools.filter(t => t.name.startsWith(\"public_\"));  \n    } else if (messageCount < 5) {\n      filteredTools = request.tools.filter(t => t.name !== \"advanced_search\");  \n    }\n\n    return handler({ ...request, tools: filteredTools });  \n  },\n});\n\nSee Dynamically selecting tools for more examples.\n​Model\nDifferent models have different strengths, costs, and context windows. Select the right model for the task at hand, which\nmight change during an agent run.\n State Store Runtime ContextUse different models based on conversation length from State:CopyAsk AIimport { createMiddleware, initChatModel } from \"langchain\";\n\n// Initialize models once outside the middleware\nconst largeModel = initChatModel(\"claude-sonnet-4-5-20250929\");\nconst standardModel = initChatModel(\"gpt-4o\");\nconst efficientModel = initChatModel(\"gpt-4o-mini\");\n\nconst stateBasedModel = createMiddleware({\n  name: \"StateBasedModel\",\n  wrapModelCall: (request, handler) => {\n    // request.messages is a shortcut for request.state.messages\n    const messageCount = request.messages.length;  \n    let model;\n\n    if (messageCount > 20) {\n      model = largeModel;\n    } else if (messageCount > 10) {\n      model = standardModel;\n    } else {\n      model = efficientModel;\n    }\n\n    return handler({ ...request, model });  \n  },\n});\n\nSee Dynamic model for more examples.\n​Response Format\nStructured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn’t sufficient.\nHow it works: When you provide a schema as the response format, the model’s final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.\n​Defining formats\nSchema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to.\nCopyAsk AIimport { z } from \"zod\";\n\nconst customerSupportTicket = z.object({\n  category: z.enum([\"billing\", \"technical\", \"account\", \"product\"]).describe(\n    \"Issue category\"\n  ),\n  priority: z.enum([\"low\", \"medium\", \"high\", \"critical\"]).describe(\n    \"Urgency level\"\n  ),\n  summary: z.string().describe(\n    \"One-sentence summary of the customer's issue\"\n  ),\n  customerSentiment: z.enum([\"frustrated\", \"neutral\", \"satisfied\"]).describe(\n    \"Customer's emotional tone\"\n  ),\n}).describe(\"Structured ticket information extracted from customer message\");\n\n​Selecting formats\nDynamic response format selection adapts schemas based on user preferences, conversation stage, or role—returning simple formats early and detailed formats as complexity increases.\n State Store Runtime ContextConfigure structured output based on conversation state:CopyAsk AIimport { createMiddleware } from \"langchain\";\nimport { z } from \"zod\";\n\nconst simpleResponse = z.object({\n  answer: z.string().describe(\"A brief answer\"),\n});\n\nconst detailedResponse = z.object({\n  answer: z.string().describe(\"A detailed answer\"),\n  reasoning: z.string().describe(\"Explanation of reasoning\"),\n  confidence: z.number().describe(\"Confidence score 0-1\"),\n});\n\nconst stateBasedOutput = createMiddleware({\n  name: \"StateBasedOutput\",\n  wrapModelCall: (request, handler) => {\n    // request.state is a shortcut for request.state.messages\n    const messageCount = request.messages.length;  \n\n    if (messageCount < 3) {\n      // Early conversation - use simple format\n      responseFormat = simpleResponse; \n    } else {\n      // Established conversation - use detailed format\n      responseFormat = detailedResponse; \n    }\n\n    return handler({ ...request, responseFormat });\n  },\n});\n\n​Tool Context\nTools are special in that they both read and write context.\nIn the most basic case, when a tool executes, it receives the LLM’s request parameters and returns a tool message back. The tool does its work and produces a result.\nTools can also fetch important information for the model that allows it to perform and complete tasks.\n​Reads\nMost real-world tools need more than just the LLM’s parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.\n State Store Runtime ContextRead from State to check current session information:CopyAsk AIimport * as z from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\nimport { createAgent } from \"langchain\";\n\nconst checkAuthentication = tool(\n  async (_, { runtime }) => {\n    // Read from State: check current auth status\n    const currentState = runtime.state;\n    const isAuthenticated = currentState.authenticated || false;\n\n    if (isAuthenticated) {\n      return \"User is authenticated\";\n    } else {\n      return \"User is not authenticated\";\n    }\n  },\n  {\n    name: \"check_authentication\",\n    description: \"Check if user is authenticated\",\n    schema: z.object({}),\n  }\n);\n\n​Writes\nTool results can be used to help an agent complete a given task. Tools can both return results directly to the model\nand update the memory of the agent to make important context available to future steps.\n State StoreWrite to State to track session-specific information using Command:CopyAsk AIimport * as z from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\nimport { createAgent } from \"langchain\";\nimport { Command } from \"@langchain/langgraph\";\n\nconst authenticateUser = tool(\n  async ({ password }, { runtime }) => {\n    // Perform authentication\n    if (password === \"correct\") {\n      // Write to State: mark as authenticated using Command\n      return new Command({\n        update: { authenticated: true },\n      });\n    } else {\n      return new Command({ update: { authenticated: false } });\n    }\n  },\n  {\n    name: \"authenticate_user\",\n    description: \"Authenticate user and update State\",\n    schema: z.object({\n      password: z.string(),\n    }),\n  }\n);\n\nSee Tools for comprehensive examples of accessing state, store, and runtime context in tools.\n​Life-cycle Context\nControl what happens between the core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.\nAs you’ve seen in Model Context and Tool Context, middleware is the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:\n\nUpdate context - Modify state and store to persist changes, update conversation history, or save insights\nJump in the lifecycle - Move to different steps in the agent cycle based on context (e.g., skip tool execution if a condition is met, repeat model call with modified context)\n\n​Example: Summarization\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in Model Context, summarization persistently updates state - permanently replacing old messages with a summary that’s saved for all future turns.\nLangChain offers built-in middleware for this:\nCopyAsk AIimport { createAgent, summarizationMiddleware } from \"langchain\";\n\nconst agent = createAgent({\n  model: \"gpt-4o\",\n  tools: [...],\n  middleware: [\n    summarizationMiddleware({\n      model: \"gpt-4o-mini\",\n      maxTokensBeforeSummary: 4000, // Trigger summarization at 4000 tokens\n      messagesToKeep: 20, // Keep last 20 messages after summary\n    }),\n  ],\n});\n\nWhen the conversation exceeds the token limit, SummarizationMiddleware automatically:\n\nSummarizes older messages using a separate LLM call\nReplaces them with a summary message in State (permanently)\nKeeps recent messages intact for context\n\nThe summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.\nFor a complete list of built-in middleware, available hooks, and how to create custom middleware, see the Middleware documentation.\n​Best practices\n\nStart simple - Begin with static prompts and tools, add dynamics only when needed\nTest incrementally - Add one context engineering feature at a time\nMonitor performance - Track model calls, token usage, and latency\nUse built-in middleware - Leverage SummarizationMiddleware, LLMToolSelectorMiddleware, etc.\nDocument your context strategy - Make it clear what context is being passed and why\nUnderstand transient vs persistent: Model context changes are transient (per-call), while life-cycle context changes persist to state\n\n​Related resources\n\nContext conceptual overview - Understand context types and when to use them\nMiddleware - Complete middleware guide\nTools - Tool creation and context access\nMemory - Short-term and long-term memory patterns\nAgents - Core agent concepts\n\nEdit the source of this page on GitHub.\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoRuntimePreviousModel Context Protocol (MCP)NextDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify",
    "metadata": {
      "source": "https://docs.langchain.com/oss/javascript/langchain/context-engineering",
      "title": "Context engineering in agents - Docs by LangChain",
      "description": "",
      "language": "en",
      "loc": "https://docs.langchain.com/oss/javascript/langchain/context-engineering",
      "lastmod": "2025-10-31T14:01:30.939Z"
    },
    "type": "Document"
  },
  {
    "page_content": "Model Context Protocol (MCP) - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationAdvanced usageModel Context Protocol (MCP)LangChainLangGraphIntegrationsLearnReferenceContributeTypeScriptOverviewLangChain v1.0Release notesMigration guideGet startedInstallQuickstartPhilosophyCore componentsAgentsModelsMessagesToolsShort-term memoryStreamingMiddlewareStructured outputAdvanced usageGuardrailsRuntimeContext engineeringModel Context Protocol (MCP)Human-in-the-loopMulti-agentRetrievalLong-term memoryUse in productionStudioTestDeployAgent Chat UIObservabilityEnglishcloseOn this pageInstallTransport typesUse MCP toolsCustom MCP serversStateful tool usageAdditional resourcesAdvanced usageModel Context Protocol (MCP)Copy pageCopy pageModel Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the langchain-mcp-adapters library.\n​Install\nInstall the @langchain/mcp-adapters library to use MCP tools in LangGraph:\nnpmpnpmyarnbunCopyAsk AInpm install @langchain/mcp-adapters\n\n​Transport types\nMCP supports different transport mechanisms for client-server communication:\n\nstdio – Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups.\nStreamable HTTP – Server runs as an independent process handling HTTP requests. Supports remote connections and multiple clients.\nServer-Sent Events (SSE) – a variant of streamable HTTP optimized for real-time streaming communication.\n\n​Use MCP tools\n@langchain/mcp-adapters enables agents to use tools defined across one or more MCP server.\nAccessing multiple MCP serversCopyAsk AIimport { MultiServerMCPClient } from \"@langchain/mcp-adapters\";  \nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { createAgent } from \"langchain\";\n\nconst client = new MultiServerMCPClient({  \n    math: {\n        transport: \"stdio\",  // Local subprocess communication\n        command: \"node\",\n        // Replace with absolute path to your math_server.js file\n        args: [\"/path/to/math_server.js\"],\n    },\n    weather: {\n        transport: \"sse\",  // Server-Sent Events for streaming\n        // Ensure you start your weather server on port 8000\n        url: \"http://localhost:8000/mcp\",\n    },\n});\n\nconst tools = await client.getTools();  \nconst agent = createAgent({\n    model: \"claude-sonnet-4-5-20250929\",\n    tools,  \n});\n\nconst mathResponse = await agent.invoke({\n    messages: [{ role: \"user\", content: \"what's (3 + 5) x 12?\" }],\n});\n\nconst weatherResponse = await agent.invoke({\n    messages: [{ role: \"user\", content: \"what is the weather in nyc?\" }],\n});\n\nMultiServerMCPClient is stateless by default. Each tool invocation creates a fresh MCP ClientSession, executes the tool, and then cleans up.\n​Custom MCP servers\nTo create your own MCP servers, you can use the @modelcontextprotocol/sdk library. This library provides a simple way to define tools and run them as servers.\nnpmpnpmyarnbunCopyAsk AInpm install @modelcontextprotocol/sdk\n\nUse the following reference implementations to test your agent with MCP tool servers.\nMath server (stdio transport)CopyAsk AIimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport {\n    CallToolRequestSchema,\n    ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\n\nconst server = new Server(\n    {\n        name: \"math-server\",\n        version: \"0.1.0\",\n    },\n    {\n        capabilities: {\n        tools: {},\n        },\n    }\n);\n\nserver.setRequestHandler(ListToolsRequestSchema, async () => {\n    return {\n        tools: [\n        {\n            name: \"add\",\n            description: \"Add two numbers\",\n            inputSchema: {\n            type: \"object\",\n            properties: {\n                a: {\n                type: \"number\",\n                description: \"First number\",\n                },\n                b: {\n                type: \"number\",\n                description: \"Second number\",\n                },\n            },\n            required: [\"a\", \"b\"],\n            },\n        },\n        {\n            name: \"multiply\",\n            description: \"Multiply two numbers\",\n            inputSchema: {\n            type: \"object\",\n            properties: {\n                a: {\n                type: \"number\",\n                description: \"First number\",\n                },\n                b: {\n                type: \"number\",\n                description: \"Second number\",\n                },\n            },\n            required: [\"a\", \"b\"],\n            },\n        },\n        ],\n    };\n});\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) => {\n    switch (request.params.name) {\n        case \"add\": {\n        const { a, b } = request.params.arguments as { a: number; b: number };\n        return {\n            content: [\n            {\n                type: \"text\",\n                text: String(a + b),\n            },\n            ],\n        };\n        }\n        case \"multiply\": {\n        const { a, b } = request.params.arguments as { a: number; b: number };\n        return {\n            content: [\n            {\n                type: \"text\",\n                text: String(a * b),\n            },\n            ],\n        };\n        }\n        default:\n        throw new Error(`Unknown tool: ${request.params.name}`);\n    }\n});\n\nasync function main() {\n    const transport = new StdioServerTransport();\n    await server.connect(transport);\n    console.error(\"Math MCP server running on stdio\");\n}\n\nmain();\n\nWeather server (SSE transport)CopyAsk AIimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { SSEServerTransport } from \"@modelcontextprotocol/sdk/server/sse.js\";\nimport {\n    CallToolRequestSchema,\n    ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\nimport express from \"express\";\n\nconst app = express();\napp.use(express.json());\n\nconst server = new Server(\n    {\n        name: \"weather-server\",\n        version: \"0.1.0\",\n    },\n    {\n        capabilities: {\n        tools: {},\n        },\n    }\n);\n\nserver.setRequestHandler(ListToolsRequestSchema, async () => {\n    return {\n        tools: [\n        {\n            name: \"get_weather\",\n            description: \"Get weather for location\",\n            inputSchema: {\n            type: \"object\",\n            properties: {\n                location: {\n                type: \"string\",\n                description: \"Location to get weather for\",\n                },\n            },\n            required: [\"location\"],\n            },\n        },\n        ],\n    };\n});\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) => {\n    switch (request.params.name) {\n        case \"get_weather\": {\n        const { location } = request.params.arguments as { location: string };\n        return {\n            content: [\n            {\n                type: \"text\",\n                text: `It's always sunny in ${location}`,\n            },\n            ],\n        };\n        }\n        default:\n        throw new Error(`Unknown tool: ${request.params.name}`);\n    }\n});\n\napp.post(\"/mcp\", async (req, res) => {\n    const transport = new SSEServerTransport(\"/mcp\", res);\n    await server.connect(transport);\n});\n\nconst PORT = process.env.PORT || 8000;\napp.listen(PORT, () => {\n    console.log(`Weather MCP server running on port ${PORT}`);\n});\n\n​Stateful tool usage\nFor stateful servers that maintain context between tool calls, use client.session() to create a persistent ClientSession.\nUsing MCP ClientSession for stateful tool usageCopyAsk AIimport { loadMCPTools } from \"@langchain/mcp-adapters/tools.js\";\n\nconst client = new MultiServerMCPClient({...});\nconst session = await client.session(\"math\");\nconst tools = await loadMCPTools(session);\n\n​Additional resources\n\nMCP documentation\n\nMCP Transport documentation\n\nlangchain-mcp-adapters\n\nEdit the source of this page on GitHub.\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoContext engineering in agentsPreviousHuman-in-the-loopNextDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify",
    "metadata": {
      "source": "https://docs.langchain.com/oss/javascript/langchain/mcp",
      "title": "Model Context Protocol (MCP) - Docs by LangChain",
      "description": "",
      "language": "en",
      "loc": "https://docs.langchain.com/oss/javascript/langchain/mcp",
      "lastmod": "2025-10-31T14:01:30.957Z"
    },
    "type": "Document"
  }
]